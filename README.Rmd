---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.path = "man/figures/README-"
)
```

<!-- badges: start -->
<!-- badges: end -->

# Overview

**TMoE** (t Mixture-of-Experts) provides a flexible and robust modelling 
framework for heterogenous data with possibly heavy-tailed distributions and 
corrupted by atypical observations. **TMoE** consists of a mixture of *K* t 
expert regressors network (of degree *p*) gated by a softmax gating network (of
degree *q*) and is represented by:

* The gating network parameters `alpha`'s of the softmax net.
* The experts network parameters: The location parameters (regression 
coefficients) `beta`'s, scale parameters `sigma`'s, and the degree of freedom 
(robustness) parameters `nu`'s. **TMoE** thus generalises mixtures of (normal, 
t, and) distributions and mixtures of regressions with these distributions. For
example, when $q=0$, we retrieve mixtures of (t-, or normal) regressions, and 
when both $p=0$ and $q=0$, it is a mixture of (t-, or normal) distributions. 
It also reduces to the standard (normal, t) distribution when we only use a 
single expert ($K=1$).

Model estimation/learning is performed by a dedicated expectation conditional 
maximization (ECM) algorithm by maximizing the observed data log-likelihood. 
We provide simulated examples to illustrate the use of the model in model-based
clustering of heterogeneous regression data and in fitting non-linear 
regression functions.

# Installation

You can install the development version of tMoE from [GitHub](https://github.com/) 
with:

```{r, eval = FALSE}
# install.packages("devtools")
devtools::install_github("fchamroukhi/tMoE")
```

To build *vignettes* for examples of usage, type the command below instead:

```{r, eval = FALSE}
# install.packages("devtools")
devtools::install_github("fchamroukhi/tMoE", 
                         build_opts = c("--no-resave-data", "--no-manual"), 
                         build_vignettes = TRUE)
```

Use the following command to display vignettes:

```{r, eval = FALSE}
browseVignettes("tMoE")
```

# Usage

```{r, message = FALSE}
library(tMoE)
```

```{r, echo = TRUE}
# Application to a simulated data set

n <- 500 # Size of the sample
alphak <- matrix(c(0, 8), ncol = 1) # Parameters of the gating network
betak <- matrix(c(0, -2.5, 0, 2.5), ncol = 2) # Regression coefficients of the experts
sigmak <- c(0.5, 0.5) # Standard deviations of the experts
nuk <- c(5, 7) # Degrees of freedom of the experts network t densities
x <- seq.int(from = -1, to = 1, length.out = n) # Inputs (predictors)

# Generate sample of size n
sample <- sampleUnivTMoE(alphak = alphak, betak = betak, sigmak = sigmak, 
                         nuk = nuk, x = x)
y <- sample$y

K <- 2 # Number of regressors/experts
p <- 1 # Order of the polynomial regression (regressors/experts)
q <- 1 # Order of the logistic regression (gating network)

n_tries <- 1
max_iter <- 1500
threshold <- 1e-5
verbose <- TRUE
verbose_IRLS <- FALSE

tmoe <- emTMoE(X = x, Y = y, K, p, q, n_tries, max_iter, 
               threshold, verbose, verbose_IRLS)

tmoe$plot()
```

```{r, echo = TRUE}
# Application to a real data set

library(MASS)
data("mcycle")
x <- mcycle$times
y <- mcycle$accel

K <- 4 # Number of regressors/experts
p <- 2 # Order of the polynomial regression (regressors/experts)
q <- 1 # Order of the logistic regression (gating network)

n_tries <- 1
max_iter <- 1500
threshold <- 1e-5
verbose <- TRUE
verbose_IRLS <- FALSE

tmoe <- emTMoE(X = x, Y = y, K, p, q, n_tries, max_iter, 
               threshold, verbose, verbose_IRLS)

tmoe$plot()
```
